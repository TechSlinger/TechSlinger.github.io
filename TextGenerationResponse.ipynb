{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechSlinger/TechSlinger.github.io/blob/main/TextGenerationResponse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMysYl9ISo-j"
      },
      "outputs": [],
      "source": [
        "!pip install -q cassandra-driver\n",
        "!pip install -q cassio>=0.1.1\n",
        "!pip install -q gradientai --upgrade\n",
        "!pip install  llama-index\n",
        "!pip install -q pypdf\n",
        "!pip install -q tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-gradient\n",
        "!pip install llama-index-embeddings-gradient\n",
        "!pip install llama-index-vector-stores-cassandra"
      ],
      "metadata": {
        "id": "pw89pPXAfAjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] ='W9Mkj70Q01rM7bQJuqS7EhrYsx1wNWhb'\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = 'c8abedd7-3556-49a9-9c28-2ad60d3f6c1d_workspace'"
      ],
      "metadata": {
        "id": "T7ccogyxTIDM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.auth import PlainTextAuthProvider\n",
        "from cassandra.cluster import Cluster\n",
        "from llama_index.core import ServiceContext\n",
        "from llama_index.core import set_global_service_context\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.embeddings.gradient import GradientEmbedding\n",
        "from llama_index.llms.gradient import GradientBaseModelLLM\n",
        "from llama_index.vector_stores.cassandra import CassandraVectorStore"
      ],
      "metadata": {
        "id": "1q-nf2ZPUVbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49F-l3DdeE3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cassandra\n",
        "print (cassandra.__version__)"
      ],
      "metadata": {
        "id": "3UHTpN4KVLRl",
        "outputId": "110c9198-bd5e-4930-b6b6-7a71bd2f07eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This secure connect bundle is autogenerated when you donwload your SCB,\n",
        "# if yours is different update the file name below\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': 'secure-connect-textai.zip'\n",
        "}\n",
        "\n",
        "# This token json file is autogenerated when you donwload your token,\n",
        "# if yours is different update the file name below\n",
        "with open(\"textAI-token.json\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "CLIENT_ID = secrets[\"clientId\"]\n",
        "CLIENT_SECRET = secrets[\"secret\"]\n",
        "\n",
        "auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "id": "PCVY4ah_YM1w",
        "outputId": "1fab2585-3e72-485d-8e3e-4af66396d531",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 9fb6f02e-7507-4b04-b227-5194f7ffde19-us-east1.db.astra.datastax.com:29042:c39a5bc8-1a0c-4e01-ab3a-883b2afa0063. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 9fb6f02e-7507-4b04-b227-5194f7ffde19-us-east1.db.astra.datastax.com:29042:c39a5bc8-1a0c-4e01-ab3a-883b2afa0063. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(133806440640960) 9fb6f02e-7507-4b04-b227-5194f7ffde19-us-east1.db.astra.datastax.com:29042:c39a5bc8-1a0c-4e01-ab3a-883b2afa0063> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 9fb6f02e-7507-4b04-b227-5194f7ffde19-us-east1.db.astra.datastax.com:29042:c39a5bc8-1a0c-4e01-ab3a-883b2afa0063. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.0.6816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GradientBaseModelLLM(\n",
        "    base_model_slug=\"llama2-7b-chat\",\n",
        "    max_tokens=400,\n",
        ")"
      ],
      "metadata": {
        "id": "aauFo7IHhdRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = GradientEmbedding(\n",
        "    gradient_access_token = os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    gradient_workspace_id = os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    gradient_model_slug=\"bge-large\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "TJX67cjehiIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    llm = llm,\n",
        "    embed_model = embed_model,\n",
        "    chunk_size=256,\n",
        ")\n",
        "\n",
        "set_global_service_context(service_context)"
      ],
      "metadata": {
        "id": "cVxcGQFMhkJF",
        "outputId": "6770383c-64d8-49d3-aeb1-63e6b01d165b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-944266470549>:1: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Documents\").load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")\n"
      ],
      "metadata": {
        "id": "klsNg1vOhr_K",
        "outputId": "48d1bbe1-4788-442e-b257-c8863a6ac3e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 20 document(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents,\n",
        "                                   service_context=service_context)\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "wnUpXcTBmiTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is the purpose of Google's operational flood forecasting system?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "uKrKeZy8msFN",
        "outputId": "98e3f2fe-c3e1-4970-97d6-aa53b8c54972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the provided context information, the purpose of Google's operational flood forecasting system is to provide accurate real-time flood warnings to agencies and the public with a focus on riverine floods in large, gauged rivers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "j3HacuDWoTFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "\n",
        "# Load the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Function to extract text from PDF files\n",
        "def extract_text_from_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Summarize a document\n",
        "document_path = \"/content/Documents/document.txt\"  # Update this with the path to your document\n",
        "file_extension = os.path.splitext(document_path)[1].lower()\n",
        "\n",
        "if file_extension == \".pdf\":\n",
        "    document_content = extract_text_from_pdf(document_path)\n",
        "elif file_extension == \".txt\":\n",
        "    with open(document_path, \"r\") as file:\n",
        "        document_content = file.read()\n",
        "else:\n",
        "    print(\"Please enter a file with a .txt or .pdf extension.\")\n",
        "    exit()\n",
        "\n",
        "# Split the document content into chunks of 1024 tokens\n",
        "max_chunk_tokens = 1024\n",
        "chunks = [document_content[i:i+max_chunk_tokens] for i in range(0, len(document_content), max_chunk_tokens)]\n",
        "\n",
        "# Summarize each chunk and concatenate the summaries\n",
        "summaries = [summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0][\"summary_text\"] for chunk in chunks]\n",
        "summary = \"\\n\".join(summaries)\n",
        "\n",
        "print(\"Your text summarize is :\\n\")\n",
        "print(summary)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rtXkXnaodfKt",
        "outputId": "830c5731-e2ca-4ef0-8694-a70e1bffa139",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 150, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your text summarize is :\n",
            "\n",
            " Machine learning algorithms and deep neural networks are being used to analyze large amounts of data and extract valuable insights . Text preprocessing is crucial to preprocess the data before using it in AI models to ensure accurate and reliable results .\n"
          ]
        }
      ]
    }
  ]
}